---
title: "Gradient Decent Animated"
date: "2015-05-01T22:12:03.284Z"
description: "Hello World"
tag: webdev
---

![](https://miro.medium.com/max/700/0*GqEabk9paWf3y2JF)

Photo by  [Lucas Clara](https://unsplash.com/@lux17?utm_source=medium&utm_medium=referral)  on  [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

> “The goal is to hit the sweet spot of maximum value optimization, where foolish risk is balanced against excessive caution.” — Steven J. Bowen

# Problem and intuition:

Most of data science algorithms are actually optimization problems, given the data and a model that we define we are trying to find the best parameters that “best fit the data”. To do so we need a way to measure the error, this comes in the form of a  **loss function** higher values indicate that our parameters estimates are poor, while lower values indicate that we are doing a great job. The problem reduces to simply finding the parameters that minimizes this loss function.

One approach would be to search all the possible parameters in our space and find which one is optimal, this could work for small problems but as the dimension of the problem increases it becomes impossible to do in a reasonable time.

Luckly, we can use some simple math concepts to greatly reduce the effort needed solve the problem. The most core of these concepts is the  **gradient.**

In simple terms the gradient of a function is a vector that points to the direction of the greatest increase so if we go in the inverse direction we will find a minimum, it might not be the best minimum.

![](https://miro.medium.com/max/700/1*UckZvoQeAxsFnA9PafTrdA.png)

IIustration of Gradient Decent — Image by Author

Throughout the article, I’ll be using some animations to illustrate how each variant of gradient decent works. The examples were generated by solving a simple linear regression problem with the Mean Squared Error as our loss function. You can get the complete code in my Github repo:

[https://github.com/tariqmassaoudi/GradientDecentAnimated](https://github.com/tariqmassaoudi/GradientDecentAnimated)

# The algorithm:

Gradient decent will start at a random point. To get to then next step it will calculate the gradient which will give us the direction, but we still need to know the size of the step to take, that where  **learning rate** comes by.

Learning rate is a coefficient we set to determine how far to go in the gradient direction a low learning rate leads to slow convergence while a big learning rate yields faster  **convergence**  but can cause us to diverge if the step is too large.

We repeat taking more steps until we reach the optimum.

![](https://miro.medium.com/max/395/0*t7fTd6eaH0QxMRU1.png)

Gradient decent steps

# Types of gradient decent

## Batch Gradient Decent:

This type of gradient decent will use all the training data to calculate the gradient. It takes stable and accurate steps but it’s costly performance wise.

![](https://miro.medium.com/max/573/1*Bl8EmB-0MVMR_EuEFa-ayw.png)

Batch Gradient Decent Error vs Number of Iterations — Image by Author

Batch Gradient Decent Linear Regression Convergence — Video By Author

Batch Gradient Decent Error Convergence — Video by Author

## Stochastic Gradient Decent:

While Batch uses all the data, stochastic will select one random point and calculates the gradient at that point. It’s not as accurate as the Batch alternative but it’s extremely fast.

![](https://miro.medium.com/max/535/1*uV4f1d68486wl1zRKFU1_A.png)

Stochastic Gradient Decent Error vs Number of Iterations — Image by Author

Stochastic Gradient Decent Linear Regression Convergence — Video By Author

Stochastic Gradient Decent Error Convergence — Video by Author

## Mini Batch Gradient Decent:

The Mini Batch approach brings the best out of both worlds, on each iteration it will randomly select several datapoints, the more points we select the more accurate each step will be.

![](https://miro.medium.com/max/542/1*GB897VLLfirwTwH__17anw.png)

Mini Batch Gradient Decent Error vs Number of Iterations Batch size of 10 — Image by Author

Mini Batch Gradient Decent Linear Regression Convergence — Video By Author

Mini Batch Gradient Decent Error Convergence — Video by Author

# What if we set a big learning rate?

Setting the learning rate too big will cause the algorithm to take big steps and diverge.

Gradient Decent Big Learning Rate — Video by Author

# Summary

Gradient decent is an optimization algorithm widely used in machine learning. The main idea behind it is going the opposite direction of the gradient until you reach a minimum.

Thanks for reading! ❤

Follow me for more informative data science content.